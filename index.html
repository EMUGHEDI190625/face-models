<!DOCTYPE html>
<html>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Face Recognition System</title>

  <!-- Load face-api.js -->
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
      padding: 10px;
    }
    video, canvas {
      border: 1px solid #ccc;
      border-radius: 8px;
      margin-top: 10px;
    }
    #status {
      margin-bottom: 10px;
      font-weight: bold;
    }
  </style>
</head>
<body>
  <h3>Face Recognition System</h3>
  <div id="status">Loading face-api.js...</div>

  <input type="text" id="employeeId" placeholder="Enter Employee ID" required><br>
  <input type="text" id="fullName" placeholder="Enter Full Name" required><br>
  <button id="start" disabled>Start Camera</button><br><br>

  <video id="video" width="320" height="240" autoplay muted></video>
  <canvas id="overlay" width="320" height="240"></canvas>

  <script>
  const statusEl = document.getElementById("status");
  const startBtn = document.getElementById("start");
  const video = document.getElementById("video");
  const canvas = document.getElementById("overlay");
  const ctx = canvas.getContext("2d");
  let labeledDescriptors = [];

  // ‚úÖ Folders
  const MODEL_URL = "./model/weights/";
  const IMAGE_PATH = "./images/";
  const SHEET_URL = "https://script.google.com/macros/s/AKfycbwg8S9MsAasSRsi-f_pH5wG-xVZylniONqshznzKCc2B2g33DkSCPmC31Yonmzoh8c/exec";

  // ‚úÖ Send recognized face info to Google Sheet
  async function sendToSheet(employeeId, fullName, imageLink) {
    const data = { employeeId, fullName, imageLink };

    try {
      await fetch(SHEET_URL, {
        method: "POST",
        body: JSON.stringify(data),
        headers: { "Content-Type": "application/json" },
      });
      console.log("‚úÖ Sent to Google Sheet:", data);
    } catch (err) {
      console.error("‚ùå Error sending to sheet:", err);
    }
  }

  // ‚úÖ Load models
  async function loadModels() {
    statusEl.textContent = "‚è≥ Loading models...";
    await Promise.all([
      faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
      faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
      faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL)
    ]);
    statusEl.textContent = "‚úÖ Models loaded. Click 'Start Camera'.";
    startBtn.disabled = false;
  }

  // ‚úÖ Load all known employee images automatically
  async function loadAllLabeledImages() {
    const employeeNames = [
      "Steve Adedayo",
      "Ikata Kenoye",
      "Jane Doe",
      "John Paul"
      // üëâ Add all employee names here (must match .jpg filenames)
    ];

    for (const name of employeeNames) {
      try {
        const img = await faceapi.fetchImage(`${IMAGE_PATH}${name}.jpg`);
        const detection = await faceapi
          .detectSingleFace(img, new faceapi.TinyFaceDetectorOptions())
          .withFaceLandmarks()
          .withFaceDescriptor();
        if (detection) {
          labeledDescriptors.push(new faceapi.LabeledFaceDescriptors(name, [detection.descriptor]));
          console.log(`‚úÖ Loaded face for ${name}`);
        } else {
          console.warn(`‚ö†Ô∏è No face detected for ${name}`);
        }
      } catch (err) {
        console.warn(`‚ö†Ô∏è Could not load ${name}:`, err);
      }
    }
  }

  // ‚úÖ Start camera & recognition
  startBtn.onclick = async () => {
    const employeeId = document.getElementById("employeeId").value.trim();
    if (!employeeId) {
      alert("Please enter Employee ID before starting.");
      return;
    }

    await loadAllLabeledImages();
    if (labeledDescriptors.length === 0) {
      alert("No known faces loaded!");
      return;
    }

    const stream = await navigator.mediaDevices.getUserMedia({ video: true });
    video.srcObject = stream;
    await video.play();

    statusEl.textContent = "üé• Camera started. Detecting faces...";
    const faceMatcher = new faceapi.FaceMatcher(labeledDescriptors, 0.6);

    setInterval(async () => {
      const detections = await faceapi
        .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
        .withFaceLandmarks()
        .withFaceDescriptors();

      ctx.clearRect(0, 0, canvas.width, canvas.height);
      faceapi.matchDimensions(canvas, { width: video.width, height: video.height });
      const resized = faceapi.resizeResults(detections, { width: video.width, height: video.height });

      const results = resized.map(d => faceMatcher.findBestMatch(d.descriptor));
      results.forEach(async (match, i) => {
        const box = resized[i].detection.box;
        const drawBox = new faceapi.draw.DrawBox(box, { label: match.toString() });
        drawBox.draw(canvas);

        if (match.label !== "unknown") {
          const recognizedName = match.label;
          const imageLink = `${location.origin}/images/${recognizedName}.jpg`;
          statusEl.textContent = `‚úÖ Recognized: ${recognizedName}`;
          await sendToSheet(employeeId, recognizedName, imageLink);
        } else {
          statusEl.textContent = "‚ùå No known face detected.";
        }
      });
    }, 1000);
  };

  window.addEventListener("load", loadModels);
</script>

</body>
</html>

